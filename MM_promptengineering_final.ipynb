{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f970bcb3-2186-47f4-9782-50f44b88416b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12.8 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os, re\n",
    "from time import ctime\n",
    "import time, math\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def mm_fewshot(model_name, readfile_name, savefile_name): \n",
    "    qconfig = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 #\"float16\"\n",
    "    )\n",
    "      \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"cuda\",\n",
    "            # torch_dtype=torch.float16,\n",
    "            quantization_config=qconfig,\n",
    "        )\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Your task is to determine if a diagnosis of multiple myeloma was explicitly stated in the given clinical note. The diagnosis must be unambiguous and must exactly mention \"myeloma\".\n",
    "    Your responses should be either \"Yes\" or \"No\". Do not respond any texts other than \"Yes\" or \"No\".\n",
    "    \n",
    "    Follow these guidelines: \n",
    "    To identify the explicit diagnosis of multiple myeloma in clinical notes, you should look for phrases or terms exactly stating \"myeloma\". Mention of relevant lab results or treatments alone does not qualify as an explicit diagnosis. Additionally:\n",
    "    Avoid mentions where multiple myeloma is considered only as a suspicion, a concern, or a differential diagnosis.\n",
    "    A history of multiple myeloma should not be identified as a current diagnosis.\n",
    "    A mention of conditions or symptoms associated with multiple myeloma should not be identified as a current diagnosis of multiple myeloma.\n",
    "    A diagnosis of MGUS or monoclonal gammopathy should not be mistaken for multiple myeloma.\n",
    "    \n",
    "    Here are some examples: \n",
    "    Respond with \"No\" when the clinical note is \"Patient has Igm monoclonal gammopathy, will repeat myeloma labs in 6 months.\"\n",
    "    Respond with \"Yes\" when the clinical note is \"Oncology diagnosis: IgG Kappa Multiple Myeloma.\"\n",
    "    Respond with \"No\" when the clinical note is  \"A/P: Patient with a hematological hx of MGUS as well as ASCVD. Etiologies of the MGUS include multiple myeloma (most likely), Amyloid and Lymphoma.\"\n",
    "    \n",
    "    Here is the clinical note: {document}\n",
    "    \"\"\"\n",
    "    \n",
    "    def llm_VAmodel(user_query):\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "        ]\n",
    "    \n",
    "        messages.extend([{\"role\":\"user\", \"content\":user_query}])\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=5, \n",
    "            temperature=0.001\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "    \n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        del model_inputs \n",
    "        del generated_ids\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return response\n",
    "    \n",
    "    df = pd.read_json(readfile_name) \n",
    "    Npatient = len(df)\n",
    "    # df2 = df.sample(frac=1, random_state=85).reset_index(drop=False).head(Npatient)\n",
    "    df2 = df #df.head(Npatient)\n",
    "    \n",
    "    resulttext = []\n",
    "    Tstart = time.time() \n",
    "    for i in range(Npatient):\n",
    "        x = df2.reportText.to_list()[i]\n",
    "        user_query = prompt_template.format(document=x)\n",
    "        answer = llm_VAmodel(user_query)\n",
    "        resulttext.append(answer)\n",
    "        print('Note '+str(i)+' done!')\n",
    "    Tend = time.time() \n",
    "    print('==== %s second =====' %(Tend-Tstart)) #('Duration:{}'.format(Tend-Tstart))\n",
    "    \n",
    "    neg_words = ['no','not','cannot']#,'not','non','negative','suspect','might','likely']\n",
    "    binaryresult = []\n",
    "    for textid in range(len(resulttext)):\n",
    "        if any(word in resulttext[textid][0:2].lower() for word in neg_words): #any(word in resulttext[textid].lower().split() for word in neg_words):\n",
    "            binaryresult.append(0)\n",
    "        else:\n",
    "            binaryresult.append(1)\n",
    "    \n",
    "    dfsavefile = pd.concat([df2.PatientSSN, df2.EntryDate, pd.DataFrame({'Output':resulttext}), pd.DataFrame({'Label':binaryresult})], axis=1)\n",
    "    dfsavefile.to_csv(savefile_name)\n",
    "    # print(dfsavefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16302dd6-42dc-4726-97af-e3091875de6a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "model_name = \".\\Llama-3.1-8B-Instruct\"\n",
    "readfile_name = r\".\\testingnotes_final.json\" \n",
    "savefile_name = \".\\llama8b\\Llama-8B-fewshot-MM-final.csv\"\n",
    "\n",
    "mm_fewshot(model_name, readfile_name, savefile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84435903-d1c4-49ba-b63b-5cdf42b91e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
