{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f970bcb3-2186-47f4-9782-50f44b88416b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.1 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os, re\n",
    "from time import ctime\n",
    "import time, math\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def mgus_fewshot(model_name, readfile_name, savefile_name):\n",
    "    qconfig = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"cuda\",\n",
    "            # torch_dtype=torch.float16,\n",
    "            quantization_config=qconfig,\n",
    "        )\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Your task is to determine if a diagnosis of monoclonal gammopathy of undetermined significance (MGUS) was explicitly stated in the given clinical note. The diagnosis must be unambiguous and must exactly mention \"MGUS\" or \"monoclonal gammopathy\".\n",
    "    Your responses should be either \"Yes\" or \"No\". Do not respond any texts other than \"Yes\" or \"No\".\n",
    "    \n",
    "    Follow these guidelines: \n",
    "    To identify the explicit diagnosis of MGUS in clinical notes, you should look for phrases or terms exactly stating \"MGUS\" or \"monoclonal gammopathy\". Mention of relevant lab results or indicators alone does not qualify as an explicit diagnosis. Additionally:\n",
    "    A serum monoclonal (M-)spike level of <=3g/dL indicates MGUS.\n",
    "    Diagnosis of multiple myeloma should not be mistaken for MGUS.\n",
    "    \n",
    "    Here are some examples:\n",
    "    Respond with \"Yes\" when the clinical note is \"On serum protein electrophoresis there is an M-spike quantitating at 2.4g.\"\n",
    "    Respond with \"Yes\" when the clinical note is \"Patient with monoclonal gammopathy IgA Lambda.\"\n",
    "    Respond with \"No\" when the clinical note is \"Patient was diagnosed with multiple myeloma after presenting with symptoms of bone pain and high calcium levels.\"\n",
    "    \n",
    "    Here is the clinical note: {document}\n",
    "    \"\"\"\n",
    "    \n",
    "    def llm_VAmodel(user_query):\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "        ]\n",
    "    \n",
    "        messages.extend([{\"role\":\"user\",\"content\":user_query}])\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=5, \n",
    "            temperature=0.001\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "    \n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        del model_inputs \n",
    "        del generated_ids\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return response\n",
    "\n",
    "    df = pd.read_json(readfile_name)\n",
    "    Npatient = len(df)\n",
    "    # df2 = df.sample(frac=1, random_state=85).reset_index(drop=False).head(Npatient)\n",
    "    df2 = df \n",
    "    \n",
    "    resulttext = []\n",
    "    Tstart = time.time() \n",
    "    for i in range(Npatient):\n",
    "        x = df2.reportText.to_list()[i]\n",
    "        user_query = prompt_template.format(document=x)\n",
    "        answer = llm_VAmodel(user_query)\n",
    "        resulttext.append(answer)\n",
    "        print('Note '+str(i)+' done!')\n",
    "    Tend = time.time() \n",
    "    print('==== %s second =====' %(Tend-Tstart)) #('Duration:{}'.format(Tend-Tstart))\n",
    "    \n",
    "    neg_words = ['no','not','cannot']#,'no,','no**','not','non','negative','suspect','might','likely']\n",
    "    binaryresult = []\n",
    "    for textid in range(len(resulttext)):\n",
    "        if any(word in resulttext[textid][0:2].lower() for word in neg_words): #any(word in resulttext[textid].lower().split() for word in neg_words):\n",
    "            binaryresult.append(0)\n",
    "        else:\n",
    "            binaryresult.append(1)\n",
    "    \n",
    "    dfsavefile = pd.concat([df2.PatientSSN, df2.EntryDate, pd.DataFrame({'Output':resulttext}), pd.DataFrame({'Label':binaryresult})], axis=1)\n",
    "    dfsavefile.to_csv(savefile_name)\n",
    "    #print(dfsavefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a70f3-a8b6-4946-bee3-3e185ddf1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \".\\Llama-3.1-8B-Instruct\"\n",
    "readfile_name = r\".\\testingnotes_final.json\" \n",
    "savefile_name = \".\\llama8b\\Llama-8B-fewshot-MGUS-final.csv\"\n",
    "\n",
    "mgus_fewshot(model_name, readfile_name, savefile_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
